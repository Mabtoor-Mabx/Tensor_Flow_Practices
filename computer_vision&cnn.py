# -*- coding: utf-8 -*-
"""Computer_vision&CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1O4BDfK-YAebEypq7sjRiKTVnBKB6QAyv

# **Import Libraries**
"""

import tensorflow as tf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tensorflow.keras.utils import plot_model

# Download The Pizza Steak Data

import zipfile
!wget https://storage.googleapis.com/ztm_tf_course/food_vision/pizza_steak.zip

# Unzip The Data
zip_ref = zipfile.ZipFile("pizza_steak.zip")
zip_ref.extractall()
zip_ref.close()

"""# **Becoming One With Data**"""

!ls pizza_steak

!ls pizza_steak/train

!ls pizza_steak/train/steak

import os 
# We Walk through pizza_steak Directories and Number of Files

for dirpath, dirnames, filenames in os.walk("pizza_steak"):
  print(f"There are {len(dirnames)} Directories and {len(filenames)} Images in {dirpath}.")

!ls -la pizza_steak

!ls -la pizza_steak/train

"""# **Another Way to find Out images in File**"""

num_steak_image_train = len(os.listdir("pizza_steak/train/steak"))

num_steak_image_train

"""# **To Visualize Images, Lets Get Class Name**"""

import pathlib 
import numpy as np
data_dir= pathlib.Path("pizza_steak/train")
class_names = np.array(sorted([item.name for item in data_dir.glob("*")]))

print(class_names)

"""# **Lets Visualize Our Images**"""

import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import random

def view_random_image(target_dir, target_class):
  
  #Set Up The Directory
  target_folder = target_dir + target_class


  # Get The Random Image  Path
  random_image = random.sample(os.listdir(target_folder), 1)
  print(random_image)


  # Read The Image & Plot it Using matplotlib
  img = mpimg.imread(target_folder + "/" + random_image[0])



  plt.imshow(img)
  plt.title(target_class)
  plt.axis("off");
  
  print(f"Image Shape : {img.shape}")

  return img  #The Reason Why We Return Img is that When it passes through mpimg function, it return in the form of array

# View Random Image For Training Datasets

img = view_random_image(target_dir="pizza_steak/train/", target_class="steak")

"""# **Lets Visualize Our Images**"""

import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import random


def view_random_image(target_dir, target_class):

  # Set Up The Directory

  target_folder = target_dir+target_class

  # Get The Random path
  random_image  = random.sample(os.listdir(target_folder), 1)

  # Read The Image and Plot it using Matplotlib

  img = mpimg.imread(target_folder + "/" + random_image[0])

  plt.imshow(img)
  plt.title(target_class)
  plt.axis("off");
  print(f" Image Shape : {img.shape} " )
  return img

# View Random Images From Training Sets

img = view_random_image(target_dir="pizza_steak/train/", target_class='steak' )

# View Random Images From Training Sets 
img = view_random_image(target_dir="pizza_steak/train/", target_class='pizza')

# View Random Images From Test Sets(Steak)

img = view_random_image(target_dir="pizza_steak/test/", target_class='pizza')

# View Random Images From test DataSets (Steak)

img = view_random_image(target_dir="pizza_steak/test/", target_class='steak')

tf.constant(img)

"""# **Build End To End CNN Model**



*   Load  Our Images
*   Preprocessing Our Images
*   Build A CNN To find Pattern To our Images 
*   Compile Our CNN
*   Fit The CNN To Our Train Data







"""

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Set The Seed
tf.random.set_seed(42)

# PreProcess The Data (Get All Data Between 0 and 1 By Scaling or Normalization )

train_datagen = ImageDataGenerator(rescale= 1./255)
valid_datagen = ImageDataGenerator(rescale=1./255)

# Set up the path to our Data Directories
train_dir = "/content/pizza_steak/train"
test_dir = "/content/pizza_steak/test"

# Import Data From Our Directories and Convert it into Batches

train_data = train_datagen.flow_from_directory(directory=train_dir,batch_size=32,target_size=(224,224),class_mode='binary', seed=42)
valid_data = valid_datagen.flow_from_directory(directory=test_dir, batch_size=32, target_size=(224,224), class_mode='binary', seed=42)


# Build The CNN Model (Same As Tiny VGG on Explainer Website)

model_1= tf.keras.models.Sequential([
                                     tf.keras.layers.Conv2D(filters=10, kernel_size=3, activation='relu', input_shape=(224,224,3)),
                                     tf.keras.layers.Conv2D(10,3, activation='relu'),
                                     tf.keras.layers.MaxPool2D(pool_size=2, padding="valid"),
                                     tf.keras.layers.Conv2D(10,3, activation='relu'),
                                     tf.keras.layers.Conv2D(10,3,activation='relu'),
                                     tf.keras.layers.MaxPool2D(2),
                                     tf.keras.layers.Flatten(),
                                     tf.keras.layers.Dense(1, activation='sigmoid')
])



# Compile The Model

model_1.compile(loss = tf.keras.losses.binary_crossentropy,
                optimizer = tf.keras.optimizers.Adam(),
                metrics=['accuracy']
              )

# Fit The Model

history_1 = model_1.fit(train_data, epochs=5, steps_per_epoch=len(train_data), validation_data=valid_data, validation_steps=len(valid_data))

#Before GPU


# Epoch 1/5
# 47/47 [==============================] - 125s 3s/step - loss: 0.5452 - accuracy: 0.7247 - val_loss: 0.3974 - val_accuracy: 0.8360



#After GPU 

# Epoch 1/5
# 47/47 [==============================] - 24s 204ms/step - loss: 0.5578 - accuracy: 0.7007 - val_loss: 0.4054 - val_accuracy: 0.8180


# Look at The Time. That is The Power Of GPU

"""# **Trying A Non CNN Model**



*   Lets Replicate Same Model But Without Using Image Dataset


"""

tf.random.set_seed(42)


model_2 = tf.keras.Sequential([
                               tf.keras.layers.Flatten(input_shape=(224,224,3)),
                               tf.keras.layers.Dense(4, activation='relu'),
                               tf.keras.layers.Dense(4, activation='relu'),
                               tf.keras.layers.Dense(1, activation='sigmoid')
])


model_2.compile(
    loss = tf.keras.losses.binary_crossentropy,
    optimizer = tf.keras.optimizers.Adam(),
    metrics=['accuracy']
)


history_2 = model_2.fit(train_data, epochs=5, steps_per_epoch= len(train_data), validation_data= valid_data, validation_steps= len(valid_data))

"""# **Try Again With Adding More layers & Epochs**"""

tf.random.set_seed(42)


model_3 = tf.keras.Sequential([
                               tf.keras.layers.Flatten(input_shape=(224,224,3)),
                               tf.keras.layers.Dense(4, activation='relu'),
                               tf.keras.layers.Dense(4, activation='relu'),
                               tf.keras.layers.Dense(4, activation='relu'),
                               tf.keras.layers.Dense(4, activation='relu'),
                               tf.keras.layers.Dense(1, activation='sigmoid')
])


model_3.compile(
    loss = tf.keras.losses.binary_crossentropy,
    optimizer = tf.keras.optimizers.Adam(),
    metrics=['accuracy']
)


history_3 = model_3.fit(train_data, epochs=100, steps_per_epoch=len(train_data), validation_data= valid_data, validation_steps= len(valid_data))

"""# **Upgrading The Model**

# Model No 4
"""

tf.random.set_seed(42)

model_4 = tf.keras.Sequential([
                               tf.keras.layers.Flatten(input_shape=(224,224,3)),
                               tf.keras.layers.Dense(100, activation='relu'),
                               tf.keras.layers.Dense(100, activation='relu'),
                               tf.keras.layers.Dense(100, activation='relu'),
                               tf.keras.layers.Dense(1, activation='sigmoid')
])

model_4.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = tf.keras.optimizers.Adam(),
    metrics=['accuracy']
)


history_4 = model_4.fit(train_data, epochs=5, steps_per_epoch=len(train_data), validation_data= valid_data, validation_steps = len(valid_data))

"""# Model No 5"""

tf.random.set_seed(42)


model_5 = tf.keras.Sequential([
                               tf.keras.layers.Flatten(input_shape=(224,224,3)),
                               tf.keras.layers.Dense(100, activation='relu'),
                               tf.keras.layers.Dense(100, activation='relu'),
                               tf.keras.layers.Dense(100, activation='relu'),
                               tf.keras.layers.Dense(1, activation='sigmoid')
])


model_5.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer  = tf.keras.optimizers.Adam(),
    metrics=['accuracy']
)

history_5 = model_5.fit(train_data, epochs=100, steps_per_epoch= len(train_data), validation_data = valid_data, validation_steps= len(valid_data))

"""# **Model No 6**"""

tf.random.set_seed(42)

model_6 = tf.keras.Sequential([
                               tf.keras.layers.Flatten(input_shape=(224,224,3)),
                               tf.keras.layers.Dense(100, activation='relu'),
                               tf.keras.layers.Dense(100, activation='relu'),
                               tf.keras.layers.Dense(100, activation='relu'),
                               tf.keras.layers.Dense(1, activation='sigmoid')
])

model_6.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = tf.keras.optimizers.Adam(),
    metrics=['accuracy']

)

history_6 = model_6.fit(train_data, epochs=50, steps_per_epoch= len(train_data), validation_data = valid_data, validation_steps = len(valid_data))

"""# **Breaking Our CNN Model**

*   Become One With Data
*   Preprocess The Data
*   Created The Model
*   Fit The Model
*   Evaluate The Model
*   Adjust Different Parameters and Improve The Model
* Repeat Until Satisfied

# **Become With Data**
"""

plt.figure()
plt.subplot(1,2,1)
steak_img = view_random_image('pizza_steak/train/', 'steak')
plt.subplot(1,2,2)
pizza_img = view_random_image('pizza_steak/train/', 'pizza')

plt.figure()
plt.subplot(1,2,1)
steak_img = view_random_image('pizza_steak/train/', 'steak')
plt.subplot(1,2,2)
pizza_img = view_random_image('pizza_steak/train/', 'pizza')

"""# **Preprocess The Data**

*  Split The Data into Training & Test Sets



"""

train_dir = 'pizza_steak/train/'
test_dir = 'pizza_steak/test/'

# Turn Data Into Batches (Batches is small Subset of datasets. Rather Than took 10,000 images, A model might look 32 at a time)
!nvidia-smi

# Create Train and Test Data Generator

from tensorflow.keras.preprocessing.image import ImageDataGenerator
train_datagen = ImageDataGenerator(rescale=1/255.)
test_datagen = ImageDataGenerator(rescale=1/255.)

# Load in Our Images Data and Turn into Batches

train_data = train_datagen.flow_from_directory(directory=train_dir, target_size=(224,224), class_mode= 'binary', batch_size=32)
test_data = test_datagen.flow_from_directory(directory= test_dir, target_size=(224,224), class_mode='binary', batch_size=32)

# Get Samples Of Train Data Batch

images, labels = train_data.next()
    # Get The Next Batch of Images/Labels
len(images), len(labels)

# How Many Batches Are There

len(train_data)

# Get The First Two Images
images[:2]

images[0].shape

images[4].shape

"""**In Deep Learning Models, There are almost Infinite Amount Of Architecture You created**

# **Created A CNN Model**

**A Baseline is Relatively Small model or Existing result that you setup when a beginning a machine Experiment, You Try to Beat The Baseline**
"""

from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Activation
from tensorflow.keras import Sequential

# Create The Model(This will be Baseliner, A layer in Convolutional Neural Network)

model_7 = Sequential([
  Conv2D(filters=10, 
         kernel_size=3, 
         strides=1,
         padding='valid',
         activation='relu', 
         input_shape=(224, 224, 3)), # input layer (specify input shape)
  Conv2D(10, 3, activation='relu'),
  Conv2D(10, 3, activation='relu'),
  Flatten(),
  Dense(1, activation='sigmoid') # output layer (specify output shape)
])

# Compile The Model

model_7.compile(loss='binary_crossentropy',
                optimizer=Adam(),
                metrics=['accuracy'])


# Fit The Model

history_7 = model_7.fit(train_data,
                        epochs=5,
                        steps_per_epoch=len(train_data),
                        validation_data=test_data,
                        validation_steps=len(test_data))

"""# **Little Explanation**

*   **Filter :**(Decides How many Times filters should pass over input tensor. Number of Sliding Window)
*   **Kernal_size :** ( Determines The Shape of Filters over Output)
*   **Padding :** (Pads The target tensors with zeros)
*   **Strides :** (Number of Steps a Filter takes across image at a time)

**In Padding:** (If Padding is Same, its mean Output shape is Same as Input shape. If Valid, Its Mean Output Shape is Compressed)

"""

# Check The Length of Training and Test Data

len(train_data), len(test_data)

"""# **Evaluate The Model**"""

import pandas as pd
pd.DataFrame(history_7.history).plot(figsize=(10,7))

pd.DataFrame(history_1.history).plot(figsize=(10,6))

pd.DataFrame(history_6.history).plot(figsize=(5,5))

"""# **Plot Validation and Training Curves Seperately**"""

def plot_loss_curves(history):

  # Return Seperate Loss Curves For Training and Test Datasets

  loss = history.history['loss']
  val_loss = history.history['val_loss']
  accuracy = history.history['accuracy']
  val_accuracy = history.history['val_accuracy']
  epochs = range(len(history.history['loss']))

  # Plot Loss

  plt.plot(epochs,loss, label='Training_Loss')
  plt.plot(epochs,loss, label='Val_loss')
  plt.title('Loss')
  plt.xlabel('Epochs')
  plt.legend()

  # Plot Accuracy
  plt.figure()
  plt.plot(epochs,loss, label='Training_accuracy')
  plt.plot(epochs,loss, label='Val_accuracy')
  plt.title('Accuracy')
  plt.xlabel('Epochs')
  plt.legend();

# Check Out The Loss And Accuracy Of Models

plot_loss_curves(history_7)

plot_loss_curves(history_2)

plot_loss_curves(history_1)

plot_loss_curves(history_5)

"""# **Adjust The Model Parameters**


**Note : When a Models Validation loss starts to Increase , It is Likelly Model is Overfitting in Training Datasets**

**fitting a Machine Learning Model Comes in 3 Steps**

1- Create A Baseline

2- Beat Baseline By Overfitting The Large Model

3- Reduce Overfitting

**Ways To Reduce Overfitting**

1-Increase Number Of Convolutional layers

2- Increase Number of Convolutional Filters

3- Add Dense Layers To Output Of our Flattend Layer



**Reduce OverFitting**

(Reduce Overfitting is Also Called Regulization)

1- Add Data Augmentation

2- Add Regulization Layer( Such as Maxpool2D)

3- Add More Data


"""

# Create The Model . (This is Going to be New Baseline)

model_8 = Sequential([
                      Conv2D(10,3, activation='relu'),
                      MaxPool2D(pool_size=2),
                      Conv2D(10,3, activation='relu'),
                      MaxPool2D(),
                      Conv2D(10, 3, activation='relu'),
                      MaxPool2D(),
                      Flatten(),
                      Dense(1, activation='sigmoid')
])


model_8.compile(
    loss = 'binary_crossentropy',
    optimizer = tf.keras.optimizers.Adam(),
    metrics=["accuracy"]
)


history_8 = model_8.fit(train_data,epochs=5, steps_per_epoch=len(train_data), validation_data = test_data, validation_steps= len(valid_data))

#Plot Loss Curves

plot_loss_curves(history_8)

"""# **Reduced Overfitting By Data Augmentation**"""

# Opening Bags of Tricks & Find Data Augmentation


train_datagen_augmented = ImageDataGenerator(rescale=1/255.,
                                             rotation_range = 0.2, 
                                             shear_range= 0.2, zoom_range= 0.2, 
                                             width_shift_range=0.2, height_shift_range=0.3,horizontal_flip=True)

train_datagen_augmented

# Create Image Datagenerator Without Data Augmentation


train_datagen = ImageDataGenerator(rescale=1/255.)

train_datagen

# Create ImageDataGenerator Without Data Augmentation For Test Data

test_datagen = ImageDataGenerator(rescale=1/255.)

test_datagen

"""# **What is Data Augmentation**


**It is the process of Altering Our Training Data Leading To have More Diversity**
"""

# Import Data and Augmented it From Training Directory

print('Augmented Training Data')

train_data_augmented = train_datagen_augmented.flow_from_directory(train_dir,
                                                                   target_size=(224,224),
                                                                   batch_size=32,
                                                                   class_mode= 'binary',
                                                                   shuffle=False)

# Create Non Augmented Data From Data Batches

print('Non Augmented Training Data')

train_data = train_datagen.flow_from_directory(train_dir,
                                               target_size=(224,224),
                                               batch_size=32,
                                               class_mode='binary',
                                               shuffle=False
                                               )

#Create Non Augmented From Data Batches

test_data = test_datagen.flow_from_directory(test_dir, target_size=(224,224), batch_size=32, class_mode='binary')

"""**Data Augmented Is Usually Performed On Training Data**"""

# Get Samples Batches

images,labels = train_data.next()
augmented_images, augmented_labels = train_data_augmented.next()

# Show Original And Augmented Image

import random
random_number = random.randint(0,32)
plt.imshow(images[random_number])
print(f"Showing Image Number : {random_number} ")
plt.title(f"Original Image")
plt.axis(False)
plt.figure()
plt.imshow(augmented_images[random_number])
plt.title(f"augmented Image")
plt.axis(False)

"""# **Train CNN Model For Augmented Train Data**"""

model_9 = Sequential([
                      Conv2D(10,3,activation='relu'),
                      MaxPool2D(pool_size=2),
                      Conv2D(10,3,activation='relu'),
                      MaxPool2D(),
                      Conv2D(10,3,activation='relu'),
                      MaxPool2D(),
                      Flatten(),
                      Dense(1,activation='sigmoid')
])

model_9.compile(
    loss = 'binary_crossentropy',
    optimizer = Adam(),
    metrics=['accuracy']
)

history_9 = model_9.fit(train_data_augmented, epochs=5, steps_per_epoch= len(train_data_augmented), validation_data= test_data, validation_steps = len(test_data))

# Check The History

plot_loss_curves(history_9)

"""# **Lets Shuffle Our Augmented Data**"""

train_data_augmented_shuffle = train_datagen_augmented.flow_from_directory(train_dir,
                                                                           target_size=(224,224),
                                                                           class_mode='binary',
                                                                           batch_size=32,
                                                                           shuffle= False)

"""# **Created The Model Same As Previous One**"""

model_10 = Sequential([
                       Conv2D(10,7, activation='relu', input_shape=(224,224,3)),
                       MaxPool2D(),
                       Conv2D(10,3, activation='relu'),
                       MaxPool2D(),
                       Conv2D(10,3,activation='relu'),
                       MaxPool2D(),
                       Flatten(),
                       Dense(1, activation='sigmoid')
])

model_10.compile(
    loss = 'binary_crossentropy',
    optimizer= Adam(),
    metrics=['accuracy']
)


history_10 = model_10.fit(train_data_augmented_shuffle, epochs=5, steps_per_epoch=len(train_data_augmented_shuffle), 
                          validation_data = test_data, validation_steps = len(test_data))

#Plot Loss Curves

plot_loss_curves(history_10)

"""**When Shuffling Training Data, The Model Gets Exposed To all Different Kinds Of Data During Enabling**

# **Repeat Until Satisfied**


**To Improve Our Model**

1- Increase Number Of Model Layers

2- Increase Number Of Filters in Convolutional layers

3- Train For Longer(More Epochs)

4- Find Ideal Learning Rate

5- Get More Data

6- Use Transfer Learning To Leverage What Another Image Model Has Learn

# **Download Custom Image To Make Predictions**
"""

#Classes We are Working with: 
class_names

#View Our Example Image

!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-steak.jpeg
steak = mpimg.imread('03-steak.jpeg')
plt.imshow(steak)
plt.axis(False)

# Check The Shape Of Image

steak.shape

"""# **Writing Helper Function To Preprocess The Image**

**When You Train a Neural Network And You want to make a prediction on your custom data , It is Important That custom data is preprocessed into Same Format as Your Model Was Trained on**
"""

# Create A Function To Import Image and Resize it with our Model

def load_and_prep_image(filename, img_shape=224):
  
  #Read In The Image
  img = tf.io.read_file(filename)

  # Decode Read File In Tensor
  img = tf.image.decode_image(img)

  # Resize The Image
  img = tf.image.resize(img, size=[img_shape, img_shape])

  # Rescale The Image(Turn it Into 0 and 1)
  img = img/255.
  return img;

# Load in and Preprocess  our Custom image

steak = load_and_prep_image('03-steak.jpeg')
steak

expanded_steak = tf.expand_dims(steak, axis=0)

model_10.predict(expanded_steak)

model_9.predict(expanded_steak)

model_1.predict(expanded_steak)

model_5.predict(expanded_steak)

pred = model_10.predict(tf.expand_dims(steak, axis=0))

"""# **Making Predictions On Custom Images**"""

# Remind Ourself for Classnames
class_names

# We can Index the Predicted Class By Rounding Prediction Probablities

pred_class = class_names[int(tf.round(pred))]

pred_class

"""# **Creating Function To Determine Whether it is Pizza/Steak or Not**"""

def pred_and_plot(model, filename, class_names= class_names):

  #  Import The Target image and Preprocess it

  img = load_and_prep_image(filename)

  # Make A Predictions

  pred = model.predict(tf.expand_dims(img, axis=0))

  # Get The Predicted Class

  pred_class = class_names[int(tf.round(pred))]

  # Plot The Image And Predicted Class

  plt.imshow(img)
  plt.title(f"Prediction : {pred_class}")
  plt.axis(False);

# Test Our Model On Custom Image
pred_and_plot(model_10, '03-steak.jpeg')

pred_and_plot(model_1,'03-steak.jpeg')

pred_and_plot(model_3, '03-steak.jpeg')

"""# **Lets Try It With Different Examples Or Pictures**"""

!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-pizza-dad.jpeg
pred_and_plot(model_10, '03-pizza-dad.jpeg')

pred_and_plot(model_1, '03-pizza-dad.jpeg')

!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-sushi.jpeg
pred_and_plot(model_10, '03-sushi.jpeg')

# !wget /content/03-pizza.jpg
# pred_and_plot(model_10, '03-pizza.jpg')

# !wget /content/03-steak-1.jpg
# pred_and_plot(model_2, '03-steak-1.jpg')

"""# **Multiclass Class Classification**





*   Become One With Data
*   Preprocess The Data
*   Create A Model(Starts With Baseline)
*   Fit The Model( Overfit it to make Sure It works)
*   Evaluate The Model
*   Adjust Different Hyperparameters & Improve The Model
*   Repeat Untill Satisfied

# **Become One With Data**
"""

import zipfile

!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_all_data.zip

# Unzip Our Data

zip_ref = zipfile.ZipFile('10_food_classes_all_data.zip', 'r')
zip_ref.extractall()
zip_ref.close()

import os

# Walk Through 10 Classes of Food Image Data

for dirpath, dirnames, filenames in os.walk('10_food_classes_all_data'):
  print(f"There are {len(dirnames)} directories and {len(filenames)}  images in '{dirpath}' ")

!ls -la 10_food_classes_all_data

# Setup The Train and Test Data Directories

train_dir = '10_food_classes_all_data/train/'
test_dir  = '10_food_classes_all_data/test/'

# Lets Get Class Name

import pathlib
import numpy as np
data_dir = pathlib.Path(train_dir)
class_names = np.array(sorted([item.name for item in data_dir.glob('*')]))
print(class_names)

# Visualize # Visualize # Visualize

import random 

img = view_random_image(target_dir= train_dir, target_class = random.choice(class_names))

"""# **Preprocess The Data**"""

from keras_preprocessing.image import image_data_generator
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Rescale
train_datagen = ImageDataGenerator(rescale=1/255.)
test_datagen = ImageDataGenerator(rescale=1/255.)

#Load Data From Directories and Turn It into Batches

train_data = train_datagen.flow_from_directory(train_dir, target_size=(224,224), batch_size = 32, class_mode='categorical')
test_data = test_datagen.flow_from_directory(test_dir, target_size=(224,224), batch_size=32, class_mode='categorical')

"""# **Created A Model (Starts With Baseline)**"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense,Flatten,MaxPool2D, Conv2D, Activation

model_11 = Sequential([
                       Conv2D(10,3, input_shape=(224,224,3)),
                       Activation(activation='relu'),
                       Conv2D(10,3, activation='relu'),
                       MaxPool2D(),
                       Conv2D(10,3,activation='relu'),
                       Conv2D(10,3,activation='relu'),
                       MaxPool2D(),
                       Flatten(),
                       Dense(10, activation='softmax')

])


model_11.compile(
    loss = 'categorical_crossentropy',
    optimizer = tf.keras.optimizers.Adam(),
    metrics=['accuracy']
)


history_11 = model_11.fit(train_data, epochs=5, steps_per_epoch= len(train_data), validation_data = test_data, validation_steps= len(test_data) )

"""# **Evaluate The Model**"""

model_11.evaluate(test_data)

# Check our Model Loss Curves 

plot_loss_curves(history_11)

"""It Seems Our Model Is Performing Overfitting

# **Adjust Model Hyperparameters**


Due to its performance on training data, it is Clear that our model is learning Something

So, Lets Try and Fix Overfitting By :  

1- Get More Data

2- Simplify The Model

3- Use Data Augmentation

4- Use Transfer Learning
"""

# How About we Try to simplify the model first

model_11.summary()

#  Lets Try to Remove 2 Convolutional Layers

model_12 = Sequential([
                       Conv2D(10,3,activation='relu', input_shape=(224,224,3)),
                       MaxPool2D(),
                       Conv2D(10,3, activation='relu'),
                       MaxPool2D(),
                       Flatten(),
                       Dense(10, activation='softmax')
])

model_12.compile(
    loss = 'categorical_crossentropy',
    optimizer = Adam(),
    metrics=['accuracy']
)


history_12 = model_12.fit(train_data, epochs=5, steps_per_epoch=len(train_data), validation_data= test_data, validation_steps = len(test_data))

# Check Out Lost Curves 

plot_loss_curves(history_12)

model_12.summary()

"""**Looks Like Our Model Didnot Work Well**

# **Data Augmentation For Reduce Overfitting**

Lets Try and improve model results by using data Augmentation

ideally we want to  :  


*   Reduce Overfitting
*   Improve Validation Accuracy
*  Create Augmented Data generator
"""

train_datagen_augmented = ImageDataGenerator(rescale=1/255. ,
                                             rotation_range=0.2,
                                             width_shift_range=0.2,
                                             height_shift_range=0.2,
                                             zoom_range=0.2,
                                             horizontal_flip = True)

train_data_augmented = train_datagen_augmented.flow_from_directory(
    train_dir,target_size=(224,224,3), batch_size=32, class_mode= 'categorical',
)

"""# **Lets Create Another Model**"""

model_13 = tf.keras.models.clone_model(model_10)

"""# **Compile The Model**"""

model_13.compile(
    loss= 'categorical_crossentropy',
    optimizer = Adam(),
    metrics=['accuracy']
)

model_13.summary()

# Fit The Model

history_13 = model_13.fit(train_data_augmented, epochs=5, steps_per_epoch=len(train_data_augmented), validation_data= test_data, validation_steps= len(test_data))

"""# **Repeat Until Satisfied**

**We could Keep here ... Continually Trying to bring TogetherOur loss curves Closer Together and Trying to Improve Validation and Test Accuracy**


How:

1- Restricting Model Architecture

2- Adusting Learning rate

3- Try Different Methods Of Data Augmentation

4- Training For Longer

5- Try Transfer Learning

# **Make Predictions With Our Trained Model**

Lets Use our Trained Data To Make Sure Predictions Of Some random Images
"""

# Remind Class Name
class_names

# Download Custom Images

!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-hamburger.jpeg
!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-pizza-dad.jpeg
!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-steak.jpeg
!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-sushi.jpeg

"""# **Function For Multiclass Image Classification**"""

def pred_and_plot(model, filename, class_names= class_names):

  #  Import The Target image and Preprocess it

  img = load_and_prep_image(filename)

  # Make A Predictions

  pred = model.predict(tf.expand_dims(img, axis=0))

  if len(pred[0]) > 1:

    pred_class = class_names[tf.argmax(pred[0])]
  else:
    
    pred_class = class_names[int(tf.round(pred[0]))]

  # Plot The Image And Predicted Class

  plt.imshow(img)
  plt.title(f"Prediction : {pred_class}")
  plt.axis(False);

# Make A Prediction Using Model_13

pred_and_plot(model=model_13, filename='03-pizza-dad.jpeg', class_names = class_names )

pred_and_plot(model= model_13, filename='03-sushi.jpeg', class_names= class_names)

pred_and_plot(model=model_13, filename = '03-hamburger.jpeg', class_names = class_names)

pred_and_plot(model=model_13, filename= '03-steak.jpeg', class_names= class_names)

"""# **Saving And Loading Our CNN Model**"""

model_13.save('save_model_13')

"""# **Load Nad Evaluate The CNN Model**"""

loaded_model = tf.keras.models.load_model('save_model_13')

# Compare Original Model and Loaded model

load_model = loaded_model.evaluate(test_data)

ori_model = model_13.evaluate(test_data)



"""# **Congartulations! This Is End Of CNN and Computer Vision**"""

